# Default values for sreagent
# This is a YAML-formatted file.

replicaCount: 1

image:
  repository: sreagent
  pullPolicy: IfNotPresent
  tag: "0.1.0"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 80
  targetPort: 8000

ingress:
  enabled: true
  className: "nginx"
  annotations: {}
  hosts:
    - host: sreagent.local
      paths:
        - path: /
          pathType: Prefix
  tls: []

resources:
  limits:
    cpu: 1000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 1Gi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}

# Application configuration
app:
  name: sreagent
  # Model provider: "gemini", "openai", "anthropic", etc.
  # All models are configured through LiteLLM (standardized approach)
  modelProvider: "gemini"
  # Model name (without provider prefix - will be formatted as "provider/model-name")
  # For Gemini: "gemini-2.0-flash", "gemini-1.5-pro", etc.
  # For OpenAI: "gpt-4", "gpt-4-turbo", "gpt-3.5-turbo", etc.
  # If you provide full format like "openai/gpt-4", it will be used as-is
  modelName: "gemini-2.0-flash"  # Default model
  host: "0.0.0.0"
  port: 8000
  # Token optimization settings (applies to all models via LiteLLM)
  maxTokens: null  # Optional: limit response tokens (e.g., 2000). null = no limit
  temperature: null  # Optional: control randomness (0.0-2.0). Lower = more focused. null = default

# MCP server configuration
mcp:
  serverHost: "sreagent-kubernetes-mcp-server"
  serverPort: 8080
  transport: "http"  # Use HTTP/SSE for remote MCP server (kubernetes-mcp-server supports SSE)

# Environment variables
env: []
  # - name: CUSTOM_VAR
  #   value: "custom_value"

# Generic Model API key configuration (standardized through LiteLLM)
# This single secret can hold either OpenAI or Gemini API key
# The agent will use the appropriate key based on modelProvider setting
model:
  # Secret name for the generic API key
  secretName: "model-api-key"
  # Key name within the secret
  secretKey: "api-key"
  # API key value (replace with real key in production)
  # This will be used for whichever provider is configured in app.modelProvider
  # In production, create manually:
  #   kubectl create secret generic model-api-key --from-literal=api-key=YOUR_KEY
  # Or update the secret:
  #   kubectl create secret generic model-api-key --from-literal=api-key=YOUR_KEY --dry-run=client -o yaml | kubectl apply -f -
  apiKey: "dummy-api-key-replace-with-real-key"

# Legacy secrets configuration (for backward compatibility)
secrets: []
  # - name: GOOGLE_APPLICATION_CREDENTIALS
  #   secretName: google-credentials
  #   secretKey: key.json

# Frontend React UI configuration
frontend:
  enabled: true
  replicaCount: 1
  image:
    repository: sreagent-frontend
    pullPolicy: IfNotPresent
    tag: "0.1.0"
  port: 3000
  service:
    type: ClusterIP
    port: 3000
  ingress:
    enabled: false
    className: "nginx"
    annotations: {}
    hosts:
      - host: sreagent-ui.local
        paths:
          - path: /
            pathType: Prefix
    tls: []
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi
  podAnnotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Kubernetes MCP Server subchart configuration
kubernetes-mcp-server:
  enabled: true
  image:
    registry: quay.io
    repository: containers/kubernetes_mcp_server
    version: "latest"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8080
  serviceAccount:
    create: true
  rbac:
    create: true
  replicaCount: 1
  ingress:
    enabled: false

